---
layout: post
cspost: yes
title:  "Neural Networks"
subheader: "Examining the building blocks of modern-day deep learning"
date:   2022-04-21
categories: tutorial
---

## Intro
With the help of linear algebra and gradients from calculus, we can build upon our intuition of how neural networks work by understanding the math behind it.

## Overview of Full Algorithm

Need to add more here

## Linear algebra

Let’s first review the basics of Linear Algebra again. This branch of mathematics provides us with some very minimal notation. Take a look at the diagram of a perceptron again. 

![Perceptron](../assets/images/nn/NNAdvancedNeuron.svg)

The output of the neuron is the weighted sum of the inputs, passed into the activation function. For now, let’s just assume the activation function is the linear activation function, $$f(x) = x$$.

$$output=w_0 + w_1x_1 + w_2x_2$$

Let's now rewrite our inputs and weights as vectors.

$$
x =
\begin{bmatrix}
x_1\\
x_2
\end{bmatrix}
\\
w=
\begin{bmatrix}
w_1\\
w_2
\end{bmatrix}
$$

If we take the dot product of these two vectors, we get the sum of an element-wise product. We set $$b$$ to equal $$w_0$$ because it's the bias.

$$
\begin{align*}
x \cdot w &= w_1x_1 + w_2x_2\\
b &= w_0\\
output &= x \cdot w + b\\
\end{align*}
$$

The notation for the weighted sum (which depends on how many inputs there are) can now be written with just three variables. The beauty here is that our notation doesn't change as our input size increases.

Let's move on to a neural network.

![Network](../assets/images/nn/FullNetwork.svg)

Our input, x, will be a vector of size four. And our first bias will be a vector of size two, because there are two neurons in the hidden layer. Our weights for each neuron in the hidden layer will be represented as rows in a matrix, $$W_1$$. The super script represents what number neuron in the hidden layer we’re on. The subscript represents which number weight for that neuron. 

$$
W_1=
\begin{bmatrix}
w^{[1]}_1 & w^{[1]}_2 & w^{[1]}_3 & w^{[1]}_4\\
w^{[2]}_1 & w^{[2]}_2 & w^{[2]}_3 & w^{[2]}_4
\end{bmatrix}
$$

Arranging the weights like so allows us to use a neat property of matrix multiplication. When we multiply $$W_1$$ by $$x$$, we'll get a vector with a length of 2. Adding that to our bias vector, $$b_1$$, grants us the values for our hidden layer.

$$
\begin{align*}
\text{hidden layer} = W_1x+b_1
\end{align*}
$$

The exact procrss is reapeated to get the values for the output layer. The weight matrix for the output layer, $$W_2$$ will be a 3 x 2 matrix. Each row in $$W_2$$ represents the weights for each neuron in the output layer. Each column represents the a neuron from the hidden layer. The output layer will have a bias vector of length 3. Let's call the hidden layer $$h$$ here.

$$\text{output layer} = W_2h+b_2$$

As a rule of thumb, the dimensions for any given weight matrix will be the number of neurons in the current layer by the number of neurons in the previous layer. The length of the bias vector will be the number of neurons in the current layer.

## Activation functions

Will come back to this. Need to add pictures of all of the actvation functions.

## Forward prop

Now that we got our basic building blocks, let’s put them all together for a example forward pass. Let's use the network from above, but add the ReLu activation function, $$R(x)$$ to the hidden layer and the sigmoid activation function, $$\sigma(x)$$, to the output layer. We start off with our input, $$x$$.

$$
\begin{align*}
z_1&=W_1x+b_1\\
a_1&=R(z_1)\\
z_2&=W_2a_1+b_2\\
\text{output}&=\sigma(z_2)
\end{align*}
$$

The key insight to gather here is the entire network is a bunch of nested functions. The result of the first linear transformation with $$W_1$$ and $$b_1$$ is the input for the ReLu function. The result of the ReLu function is the input for the second linear transofrmation, and so on.

If we really wanted to, we could write the whole network on one line. It's probably clear why we don't though.

$$\text{output layer}=\sigma(W_2(R(W_1x+b_1))+b_2)$$


This will come in handy when we start to calculate the derivatives soon.

## Cost functions

Need to come back to this. Will talk about all possible cost functions
### MSE

## Backward prop

This is the complicated part of a neural network, the training process. I’ve redrawn the neural network from above in a slightly different manner. This format will help us understand the calculus more easily.

![Network](../assets/images/nn/ReformattedNetwork.svg)

The colors represent each layer and each rectangle represents an operation that’s performed in the network. The output of one function is the input for the next one. This extends the insight of how a neural network is a bunch of nested functions.

### Chain Rule Review

Imagine we have the following two functions, and we're trying to find the derivative of $$f(g(x))$$. 

$$
\begin{align*}
f(x)&=x^2\\
g(x)&=5x+3\\
f(g(x))&=(5x+3)^2
\end{align*}
$$

The chain rule dictates how to take the derivative of nested functions; You derive the outer one, while treating the inner function as a variable. Then, you multiply the result by the derivative of the second. So the derivative of $$f(g(x))$$ is $$f'(g(x))g'(x)$$.
$$
\begin{align*}
f'(x)&=2x\\
g'(x)&=5\\
f'(g(x))&=2(5x+3)*5\\
&=10(5x+3)\\
&=50x+30
\end{align*}
$$

