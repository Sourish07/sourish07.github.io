---
layout: post
cspost: yes
title:  "Neural Networks"
subheader: "Examining the building blocks of modern-day deep learning"
date:   2022-04-21
categories: tutorial
---

## Intro
With the help of linear algebra and gradients from calculus, we can build upon our intuition of how neural networks work by understanding the math behind it.

## Overview of Full Algorithm

Need to add more here

## Linear algebra

Let’s first review the basics of Linear Algebra again. This branch of mathematics provides us with some very minimal notation. Take a look at the diagram of a perceptron again. 

![Perceptron](../assets/images/nn/NNAdvancedNeuron.svg)

The output of the neuron is the weighted sum of the inputs, passed into the activation function. For now, let’s just assume the activation function is the linear activation function, $$f(x) = x$$.

$$output=w_0 + w_1x_1 + w_2x_2$$

Let's now rewrite our inputs and weights as vectors.

$$
x =
\begin{bmatrix}
x_1\\
x_2
\end{bmatrix}
\\
w=
\begin{bmatrix}
w_1\\
w_2
\end{bmatrix}
$$

If we take the dot product of these two vectors, we get the sum of an element-wise product. We set $$b$$ to equal $$w_0$$ because it's the bias.

$$
\begin{align*}
x \cdot w &= w_1x_1 + w_2x_2\\
b &= w_0\\
output &= x \cdot w + b\\
\end{align*}
$$

The notation for the weighted sum (which depends on how many inputs there are) can now be written with just three variables. The beauty here is that our notation doesn't change as our input size increases.

Let's move on to a neural network.

![Network](../assets/images/nn/FullNetwork.svg)

Our input, x, will be a vector of size four. And our first bias will be a vector of size two, because there are two neurons in the hidden layer. Our weights for each neuron in the hidden layer will be represented as rows in a matrix, $$W_1$$. The super script represents what number neuron in the hidden layer we’re on. The subscript represents which number weight for that neuron. 

$$
W_1=
\begin{bmatrix}
w^{[1]}_1 & w^{[1]}_2 & w^{[1]}_3 & w^{[1]}_4\\
w^{[2]}_1 & w^{[2]}_2 & w^{[2]}_3 & w^{[2]}_4
\end{bmatrix}
$$

Arranging the weights like so allows us to use a neat property of matrix multiplication. When we multiply $$W_1$$ by $$x$$, we'll get a vector with a length of 2. Adding that to our bias vector, $$b_1$$, grants us the values for our hidden layer.

$$
\begin{align*}
\text{hidden layer} = W_1x+b_1
\end{align*}
$$

The exact procrss is reapeated to get the values for the output layer. The weight matrix for the output layer, $$W_2$$ will be a 3 x 2 matrix. Each row in $$W_2$$ represents the weights for each neuron in the output layer. Each column represents the a neuron from the hidden layer. The output layer will have a bias vector of length 3. Let's call the hidden layer $$h$$ here.

$$\text{output layer} = W_2h+b_2$$

As a rule of thumb, the dimensions for any given weight matrix will be the number of neurons in the current layer by the number of neurons in the previous layer. The length of the bias vector will be the number of neurons in the current layer.

## Activation functions

Will come back to this. Need to add pictures of all of the actvation functions.

## Forward prop

Now that we got our basic building blocks, let’s put them all together for a example forward pass. Let's use the network from above, but add the ReLu activation function, $$R(x)$$ to the hidden layer and the sigmoid activation function, $$\sigma(x)$$, to the output layer. We start off with our input, $$x$$.

$$
\begin{align*}
z_1&=W_1x+b_1\\
a_1&=R(z_1)\\
z_2&=W_2a_1+b_2\\
\text{output}&=\sigma(z_2)
\end{align*}
$$

The key insight to gather here is the entire network is a bunch of nested functions. The result of the first linear transformation with $$W_1$$ and $$b_1$$ is the input for the ReLu function. The result of the ReLu function is the input for the second linear transofrmation, and so on.

If we really wanted to, we could write the whole network on one line. It's probably clear why we don't though.

$$\text{output layer}=\sigma(W_2(R(W_1x+b_1))+b_2)$$


This will come in handy when we start to calculate the derivatives soon.

## Cost functions

Need to come back to this. Will talk about all possible cost functions
### MSE

## Backward prop

This is the complicated part of a neural network, the training process. I’ve redrawn the neural network from above in a slightly different manner. This format will help us understand the calculus more easily.

![Network](../assets/images/nn/ReformattedNetwork.svg)

The colors represent each layer and each rectangle represents an operation that’s performed in the network. The output of one function is the input for the next one. We pass $$x$$ into the first linear transormation into, $$z_1(x, W_1, b_1)$$, then into the ReLu activation function, then into the second linear transformation, $$z_2(a_1, W_2, b_2)$$, then into the sigmoid activation function, and finally into the loss function to calculate the error.

This extends the insight of how a neural network is a bunch of nested functions.

### Chain Rule Review

Imagine we're trying to find the derivative of $$h(x)$$, which is comprised of two functions, one nested in the other, i.e. $$h(x)=f(g(x))$$, or the output of $$g(x)$$ is the input of $$f(x)$$.

Let's say $$f(x)$$ and $$g(x)$$ are equal to the equations below.

$$
\begin{align*}
f(x)&=x^2\\
g(x)&=5x+3\\
h(x)&=(5x+3)^2
\end{align*}
$$

The chain rule dictates how to take the derivative of nested functions; You derive the outer one, while treating the inner function as a variable. Then, you multiply the result by the derivative of the inner function. So the derivative of $$h(x)$$ is $$f'(g(x))g'(x)$$. In other words, $$\frac{dh}{dx}=\frac{df}{dg}\frac{dg}{dx}$$.
$$
\begin{align*}
f'(x)&=2x\\
g'(x)&=5\\
h'(x)&=2(5x+3)*5\\
&=10(5x+3)\\
&=50x+30
\end{align*}
$$

### Partial Derivatives Review

Let's say we have the following equation, $$f(x) = x^2y$$. Partial derivatives represent the change in the output of the equation when one of the input variables are changed by a small amount. The partial derivative of $$f(x)$$ is how the output of the function will be affected when changing $$x$$ by a small while keeping $$y$$ constant. The partial derivative with respect to $$y$$ is how the output of the function will be affected when changing $$y$$ by a small while keeping $$x$$ constant. 

To calculate the partial derivatives, we take the derivative like normal, but pretend one of the variables is just like any other constant.

$$
\begin{align*}
\frac{\partial f}{\partial x}&=2xy\\
\frac{\partial f}{\partial y}&=x^2
\end{align*}
$$

When we stack all of the partial derivatives into a vector, we call that the gradient vector of a function.
$$\nabla f=
\begin{bmatrix}
2xy\\
x^2
\end{bmatrix}
$$

<hr>

There are four values in total we want to calculate, $$\frac{\partial \mathcal{L}}{\partial W_1}$$, $$\frac{\partial \mathcal{L}}{\partial b_1}$$, $$\frac{\partial \mathcal{L}}{\partial W_2}$$, and $$\frac{\partial \mathcal{L}}{\partial b_2}$$.

These are the four parameters of our network, the ones we want to optimize. We can now use the chain rule to write the equations for each of these.

$$
\begin{align*}
\frac{\partial \mathcal{L}}{b_2}&=\frac{\partial \mathcal{L}}{\partial a_2}\frac{\partial a_2}{\partial z_2}\frac{\partial z_2}{\partial b_2}\\
\frac{\partial \mathcal{L}}{W_2}&=\frac{\partial \mathcal{L}}{\partial a_2}\frac{\partial a_2}{\partial z_2}\frac{\partial z_2}{\partial W_2}\\
\frac{\partial \mathcal{L}}{b_1}&=\frac{\partial \mathcal{L}}{\partial a_2}\frac{\partial a_2}{\partial z_2}\frac{\partial z_2}{\partial a_1}\frac{\partial a_1}{\partial z_1}\frac{\partial z_1}{\partial b_1}\\
\frac{\partial \mathcal{L}}{b_1}&=\frac{\partial \mathcal{L}}{\partial a_2}\frac{\partial a_2}{\partial z_2}\frac{\partial z_2}{\partial a_1}\frac{\partial a_1}{\partial z_1}\frac{\partial z_1}{\partial W_1}
\end{align*}
$$

If you look closely, you'll see a lot of repeated partials across all of the equations, such as the product $$\frac{\partial \mathcal{L}}{\partial a_2}\frac{\partial a_2}{\partial z_2}$$. This is conveient for us because it means we can just compute them once and store them for later use.

Just as a reminder, $$\frac{\partial a_1}{\partial z_1}$$ and $$\frac{\partial a_2}{\partial z_2}$$ are just the derivatives of the ReLu and sigmoid functions, respectively.

### Calculating the Partials

The premise of the back propagation algorithm is that we need to take the partial derivative of the cost function with respect to each of the weight matrices and bias vectors. Given that the loss function is comprised of several nested functions, we'll be needing to utilize the chain rule throughout our computations.

In order to calculate all of the partials, we'll need to find the derivatives of each of the cells in our diagram from above. Then, we'll need to multiply the appropriate results for each partial we're trying to find.

Let's start off with our cost function, Mean Squared Error. Because $$\hat y$$ is the value that's dependant on our weights and biases, we want to take the partial derivative with respect to that.

$$
\begin{align*}
\mathcal{L}(a_2)&=\frac{(y-a_2)^2}{2}\\
\frac{\partial \mathcal{L}}{\partial \hat y}&=-(y-a_2)
\end{align*}
$$

Moving on to the derivative of the sigmoid. I won't actually derive the whole thing here, but the result is inserted below.

$$\sigma'(z_2)=\sigma(z_2)(1-\sigma(z_2))$$

Now, let's calculate the partial of the second linear transformation, $$W_2a_1+b_2$$, with respect to $$W_2$$ because we want to optimize our weight matrices. It's just $$a_1$$. The partial with respect to our bias vector is just $$1$$. And our partial with respect to $$a_1$$ is $$W_2$$; We'll need this partial as we propagate further back for $$W_1$$ and $$b_1$$.

We have all of the pieces of our equation to write the complete partial derivatives for $$W_2$$ and $$b_2$$.

$$
\begin{align*}
\frac{\partial \mathcal{L}}{W_2}&=\frac{\partial \mathcal{L}}{\partial a_2}\frac{\partial a_2}{\partial z_2}\frac{\partial z_2}{\partial W_2}\\
\frac{\partial \mathcal{L}}{\partial a_2}&=-(y-a_2)\\
\frac{\partial a_2}{\partial z_2}&=\sigma'(z_2)\\
\frac{\partial z_2}{\partial W_2}&=a_1\\
\frac{\partial \mathcal{L}}{W_2}&=-(y-a_2)\sigma'(z_2)a_1
\end{align*}
$$

The partial with respect to our bias is very similar.
$$
\begin{align*}
\frac{\partial \mathcal{L}}{b_2}&=\frac{\partial \mathcal{L}}{\partial a_2}\frac{\partial a_2}{\partial z_2}\frac{\partial z_2}{\partial b_2}\\
\frac{\partial z_2}{\partial b_2}&=1\\
\frac{\partial \mathcal{L}}{b_2}&=-(y-a_2)\sigma'(z_2)
\end{align*}
$$

The derivative of the ReLu function is a little bit unorthodox, just because it's a piecewise.

$$
\frac{\partial R}{\partial z_1}=
    \begin{cases}
        0 & \text{if } z_1 \leq 0\\
        1 & \text{if } z_1 > 0
    \end{cases}
$$

And the derivatives of the first linear transformation is the same as the one previously computed. The derivative with respect to $$W_1$$ is $$x$$ and the derivative with respect to $$b_1$$ is $$1$$.

$$
\begin{align*}
\frac{\partial \mathcal{L}}{W_1} &= \frac{\partial \mathcal{L}}{\partial a_2}\frac{\partial a_2}{\partial z_2}\frac{\partial z_2}{\partial a_1}\frac{\partial a_1}{\partial z_1}\frac{\partial z_1}{\partial W_1}\\
\frac{\partial z_2}{\partial a_1} &= W_2\\
\frac{\partial a_1}{\partial z_1} &= \begin{cases}
        0 & \text{if } z_1 \leq 0\\
        1 & \text{if } z_1 > 0
    \end{cases}\\
\frac{\partial z_1}{\partial W_1} &= x\\
\frac{\partial \mathcal{L}}{W_1}&=-(y-a_2)\sigma'(z_2)W_2R'(z_2)x
\end{align*}
$$

$$
\begin{align*}
\frac{\partial \mathcal{L}}{b_1}&=\frac{\partial \mathcal{L}}{\partial a_2}\frac{\partial a_2}{\partial z_2}\frac{\partial z_2}{\partial a_1}\frac{\partial a_1}{\partial z_1}\frac{\partial z_1}{\partial b_1}\\
\frac{\partial z_1}{\partial b_1} &= 1\\
\frac{\partial \mathcal{L}}{b_1}&=-(y-a_2)\sigma'(z_2)W_2R'(z_2)
\end{align*}
$$







$$
\begin{align*}
\end{align*}
$$
